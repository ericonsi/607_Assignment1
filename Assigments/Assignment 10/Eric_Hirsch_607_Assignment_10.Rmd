---
title: "Eric_Hirsch_607_Assigment_10"
author: "Eric Hirsch"
date: "4/11/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## {.tabset .tabset-pills}

### Chapter Summary Example

```{r}
#install.packages('tidytext')
#install.packages('psych')
library(tidytext)
library(tidyverse)
library(janeaustenr)
library(dplyr)
library(stringr)
library(tidyr)
library(wordcloud)
library(reshape2)
library(magrittr)
library(openintro)
library(psych)


```



```{r get sentiments}
get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")

```

#### Joy Words In Emma
```{r load Jane Ausitn}
a<-austen_books()

tidy_books <- austen_books() %>%
  group_by(book) %>%
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, 
                                regex("^chapter [\\divxlc]", 
                                      ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)
```

```{r Get Sentiments - Jane Austin}

nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

tidy_books %>%
  filter(book == "Emma") %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)

```

#### Jane Austin Sentiment Analysis Throughout her Novels

```{r group by sections}


jane_austen_sentiment <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative)

```

```{r Plot it}

ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")

```


#### Comparing the Dictionaries

#### Comparing results of Pride and Prejudice

Same peaks and dips but NRC is more positive

```{r}

pride_prejudice <- tidy_books %>% 
  filter(book == "Pride & Prejudice")

pride_prejudice

afinn <- pride_prejudice %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = linenumber %/% 80) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")

bing_and_nrc <- bind_rows(
  pride_prejudice %>% 
    inner_join(get_sentiments("bing")) %>%
    mutate(method = "Bing et al."),
  pride_prejudice %>% 
    inner_join(get_sentiments("nrc") %>% 
                 filter(sentiment %in% c("positive", 
                                         "negative"))
    ) %>%
    mutate(method = "NRC")) %>%
  count(method, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>% 
  mutate(sentiment = positive - negative)

bind_rows(afinn, 
          bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")

```

#### Positive and negative words per lexicon

```{r Pos and ne}

get_sentiments("nrc") %>% 
  filter(sentiment %in% c("positive", "negative")) %>% 
  count(sentiment)

get_sentiments("bing") %>% 
  count(sentiment)


```


#### Most Common Postive and Negative Words

```{r}

bing_word_counts <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts

bing_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)
```

#### Add a word to the stop-words list

```{r stop words}

custom_stop_words <- bind_rows(tibble(word = c("miss"),  
                                      lexicon = c("custom")), 
                               stop_words)

custom_stop_words


```

#### Word Clouds

```{r word clouds}

tidy_books %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))

tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)

```

#### Looking at Units Beyond Just Words

```{r sentences}

p_and_p_sentences <- tibble(text = prideprejudice) %>% 
  unnest_tokens(sentence, text, token = "sentences")
p_and_p_sentences$sentence[2]

```

``` {r regex}
austen_chapters <- austen_books() %>%
  group_by(book) %>%
  unnest_tokens(chapter, text, token = "regex", 
                pattern = "Chapter|CHAPTER [\\dIVXLC]") %>%
  ungroup()

austen_chapters %>% 
  group_by(book) %>% 
  summarise(chapters = n())

```

#### We will use tidy text analysis to ask what are the most negative chapters in each of Jane Austenâ€™s novels 

1. Get the list of negative words from the Bing lexicon. 
2. Make a data frame of how many words are in each chapter so we can normalize for the length of chapters. 
3. Find the number of negative words in each chapter and divide by the total words in each chapter. 

```{r  Chapter analysis}

bingnegative <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")

wordcounts <- tidy_books %>%
  group_by(book, chapter) %>%
  summarize(words = n())

tidy_books %>%
  semi_join(bingnegative) %>%
  group_by(book, chapter) %>%
  summarize(negativewords = n()) %>%
  left_join(wordcounts, by = c("book", "chapter")) %>%
  mutate(ratio = negativewords/words) %>%
  filter(chapter != 0) %>%
  slice_max(ratio, n = 1) %>% 
  ungroup()

```

### Sentiment Analysis On Broadway

I am analyzing a dataset from kaggle (https://www.kaggle.com/jessemostipak/broadway-weekly-grosses) which has a table of synopses and a table of gross receipts for Broadway plays for the past several decades. I will use the package SentimentAnalysis, which contains 4 dictionaries: Harvard-IV, an all-purpose dictionary developed at Harvard, Henry's finance specific dictionary, the Loughran-Macdonald dictionary (another finance dictionary), and the QDAP, which analyses discourse.  Given the specific foci of the latter three databases, I expect the Harvard database to be the most useful.

The reason I am using the SentimentAnalysis package is that it operates differently from the tidytext library - it provides summary statistics for weighted scores using a complex algorithm (utilizing LASSO regularization) which considers not only a term within a discourse but its position.  This may help us more reliably determine whether an observation is predominantly positive or negative.

```{r load data}
library(SentimentAnalysis)
dfGrosses<-read.csv("https://raw.githubusercontent.com/ericonsi/CUNY_607/main/Assigments/Assignment%2010/grosses.csv")
dfSynopses<-read.csv("https://raw.githubusercontent.com/ericonsi/CUNY_607/main/Assigments/Assignment%2010/synopses.csv", fileEncoding = "UTF-8-BOM")

```

#### We compare the performance of the 4 dictionaries on a simple pass-through of the play synopses:


```{r jhg}

sentiment <- analyzeSentiment(dfSynopses$synopsis)

table(convertToBinaryResponse(sentiment$SentimentGI))
summary(sentiment$SentimentGI)
ggplot(sentiment, aes(SentimentGI)) +
  geom_histogram() +
  ggtitle("Harvard-IV")

table(convertToBinaryResponse(sentiment$SentimentGI))
summary(sentiment$SentimentHE)
ggplot(sentiment, aes(SentimentHE)) +
  geom_histogram() +
   ggtitle("Henry")


table(convertToBinaryResponse(sentiment$SentimentGI))
summary(sentiment$SentimentLM)
ggplot(sentiment, aes(SentimentLM)) +
  geom_histogram() +
   ggtitle("LM")


table(convertToBinaryResponse(sentiment$SentimentGI))
summary(sentiment$SentimentQDAP)
ggplot(sentiment, aes(SentimentQDAP)) +
  geom_histogram() +
   ggtitle("QDAP") 
```


Not surprisingly, in the finance dictionaries there are few matches between the dictionaries and words in the play synopses.  The QDAP does better, and the Harvard analysis has the most matches. The QDAP analysis and the Harvard analysis are relatively similar in their distribution.  They both find the plays, on average, to be mildly positive.

#### We compare the Harvard dictionary to the dictionaries we used from Tidytext.


```{r}

summary(DictionaryGI)

get_sentiments("nrc") %>% 
  filter(sentiment %in% c("positive", "negative")) %>% 
  count(sentiment)

get_sentiments("bing") %>% 
  count(sentiment)
```


The Harvard dictionary (DictionaryGI in the r code) has fewer words than the nrc and bing dictionaries.  In addition, there is much more parity between positive and negative words in the Harvard dictionary than the others.


#### Using sentiment analysis to predict show success

Now that we have a simple tool for evaluating synopsis sentiment we can use it to do some analysis.  For example, is there an association between play sentiment and gross revenue or seats sold?

The boxplots and scatterplots below suggest there is.

```{r seats sold}

dfSynopses2 <- cbind(dfSynopses, sentiment)

dfTotalSeats <- dfGrosses %>%
  group_by(show) %>%
  summarise(total_seats_sold  = sum(seats_sold), total_revenues = sum(weekly_gross))

dfJoin <- dfSynopses2 %>% 
  inner_join(dfTotalSeats, by="show") 

dfJoin <- na.omit(dfJoin)

dfSeatAnalysis <- dfJoin %>%
  filter(total_seats_sold<1000000) %>%
  filter(total_revenues>20000000) %>%
  mutate(Sentiment_Valence = case_when(SentimentGI > 0 ~ 'positive',
                           SentimentGI <=0 ~ 'neutral or negative')) 

```


```{r scatter}

ggplot(dfSeatAnalysis, aes(x =Sentiment_Valence, y=total_seats_sold)) +
  geom_boxplot()

ggplot(dfSeatAnalysis, aes(SentimentGI, total_seats_sold)) +
  geom_point() +
  stat_smooth(method = "lm", se = FALSE)

ggplot(dfSeatAnalysis, aes(x =Sentiment_Valence, y=total_revenues)) +
  geom_boxplot()

ggplot(dfSeatAnalysis, aes(SentimentGI, total_revenues)) +
  geom_point() +
  stat_smooth(method = "lm", se = FALSE)

```

While the data remains suggestive that more positive shows do better, regression analysis fails to allow us to reject the possibility that the apparent association is due to sampling error (at a 95% confidence level.)

```{r regressions}
m3 <- lm(total_seats_sold ~ SentimentGI, data = dfSeatAnalysis)
summary(m3)

m <- lm(total_revenues ~ SentimentGI, data = dfSeatAnalysis)
summary(m)
```

#### Comparing the Harvard results and the Bing results

Here we analyze the synopses dataset using Bing to see if it makes the same conclusions about the plays' sentiment. 

```{r  abc}

tidy_x <- dfSynopses %>%
  unnest_tokens(word, synopsis)

bing_word_counts2 <- tidy_x %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

x <- bing_word_counts2 %>%
  count(sentiment) %>%
  mutate(percentage = n / sum(n))
x
```
The Bing dataset comes to the astonishing conclusion that 60% of Broadway plays are negative in sentiment! This is not the Broadway I know.

This word cloud helps us understand why:

``` {r fjl}
tidy_x %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```

Consider this: A satirical farce about an eccentric millionaire who falls intensely in love with a mysterious stranger has 7 negative words and only one positive one.  Even "funny" is considered negative here.

It does not help all that much to remove some of the words:

```{r gg}

bing_word_counts3 <- tidy_x %>%
  filter(word !="falls" & word !="fall" & word!="unexpected" & word!="mysterious" & word!="farce") %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

x1 <- bing_word_counts3 %>%
  count(sentiment) %>%
  mutate(percentage = n / sum(n))
x1

```

#### Conclusion

Sentiment analysis is an interesting tool but has its limitations.  As the Bing vs Harvard analysis shows, different dictionaries and algorithms can produce different results.  As more context-specific dictionaries emerge, analyses will surely improve.


