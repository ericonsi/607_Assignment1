---
title: "Poverty and Diabetes: Is There A Connection?"
Author: "Eric Hirsch"
output: 
  html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r ech=FALSE, message = FALSE}
#install.packages("janitor")
library(tidyverse)
library(magrittr)
library(janitor)
library(tidyverse)
library(RODBC)
library(pscl)
library(caret)
library(car)
library(InformationValue)
library(rvest)
library(jsonlite)
library(xml2)
library(XML)
library(stringr)
```
## Diabetes and Poverty: Is There a Connection?

We will use data from 4 sources: 

1. Health data from the New York Department of Health, accessed through APIs
2. Zip code level data on population size and poverty, scraped from web tables
3. CsV files from a proprietary survey of economic needs (stripped of identity markers) 
4. Tables of client health information in a proprietary SQL Server database on Azure (stripped of identity markers and password protected through keyring)

Note - Certain operations that load data and are very time consuming will be contained within functions that can be run or not by changing the value of the boolean variable "run" which appears at the top of each function.

## Loading The Data

### 1. New York DOH Data

We begin by reading the New York DOH data from the APIs:
```{r}
library("RSocrata")

#This operation is time consuming.  Set run to TRUE the first time you run ii, and FALSE afterward.
run=FALSE 

if(run) {
#Community Health: Diabetes mortality rate per 100,000: 2009 - 2011
dfMortality2011 <- read.socrata(
  "https://health.data.ny.gov/resource/u98s-c3hg.json")

#U.S. Chronic Disease Indicators: Diabetes
dfDiabetesIndicators <- read.socrata(
  "https://chronicdata.cdc.gov/resource/f8ti-h92k.json")

#Community Health Obesity and Diabetes Related Indicators: 2008 - 2012
dfCommunityHealthObesityAndDiabetes <- read.socrata(
  "https://health.data.ny.gov/resource/tchg-ruva.json")

#Community Health: Age-adjusted percentage of adults with physician diagnosed diabetes: 2008 - 2009
dfAdultsDiagnosed <- read.socrata(
  "https://health.data.ny.gov/resource/9j5w-7zpd.json")

#AH Provisional Diabetes Death Counts, 2020
dfDeath <- read.socrata(
  "https://data.cdc.gov/resource/qdcb-uzft.json")

#500 Cities: Diagnosed diabetes among adults aged >=18 years
dfCityComparison <- read.socrata(
  "https://chronicdata.cdc.gov/resource/cn78-b9bj.json")

#Conditions contributing to deaths involving coronavirus disease 2019 (COVID-19), by age group and state, United States.
dfDeathsForCovid <- read.socrata(
  "https://data.cdc.gov/resource/hk9y-quqm.json")

#Community Health: Diabetes Short-term Complications Hospitalization Rate per 10,000 - Aged 18+ Years by County Map: Latest Data
dfHospitalizations <- read.socrata(
  "https://health.data.ny.gov/resource/xuwq-ppg8.json")

#Medicaid Chronic Conditions, Inpatient Admissions and Emergency Room Visits by Zip Code: Beginning 2012
dfMedicaidByZip <- read.socrata(
  "https://health.data.ny.gov/resource/2yck-xisk.json")

}

```
```{r}
#This operation is time consuming.  Set run to TRUE the first time you run ii, and FALSE afterward.
run=FALSE 

if(run) {
#Medicaid Program Enrollment by Month: Beginning 2009
dfMedicaidEnrollment <- read.socrata(
  "https://health.data.ny.gov/resource/m4hz-kzn3.json")
}
```


### 2. Zip Code Level Population and Poverty Data Scraped From the WEb

We scrape 17 pages of zipatlas data to compile a table of zip codes, population levels and poverty levels.

```{r html}
library(XML)

#This operation is time consuming.  Set run to TRUE the first time you run ii, and FALSE afterward.
run=FALSE

if(run) {


x="http://www.zipatlas.com/us/ny/zip-code-comparison/population-below-poverty-level.htm"
dfPopAndPovertyLevel = as.data.frame(readHTMLTable(x, header=T,which=5,strings2factors=F))

for (i in 1:16) {
x= str_c("http://www.zipatlas.com/us/ny/zip-code-comparison/population-below-poverty-level.", i, ".htm")
dfx = as.data.frame(readHTMLTable(x, header=T,which=5,strings2factors=F))
dfPopAndPovertyLevel=rbind(dfPopAndPovertyLevel, dfx)
}
}

#This operation is time consuming.  Set run to TRUE the first time you run ii, and FALSE afterward.
run=FALSE

if(run) {
  
dfPopAndPovertyLevel %<>%
  filter(str_length(V2)==5 & !is.na(V4)) %<>%
  rename(number = V1, zip=V2, location=V3, city=V4, population=V5, percent_below_poverty=V6, rank=V7) %<>%
  mutate(population = as.numeric(gsub(",", "", population))) %<>%
  mutate(percent_below_poverty = as.numeric(gsub(" %", "", percent_below_poverty))) %<>%
  mutate(rank = as.numeric(gsub("#", "", rank)))

}

```


### 3. CSV file of a Survey of Client Economic Needs

The clients in this file are clients from my workplace and their Client ID is the same as for those at my workplace.  The file is otherwise free of any personal identifying information.   


```{r}

#This operation is time consuming.  Set run to TRUE the first time you run ii, and FALSE afterward.
run=FALSE

if(run) {
dfSDOH_raw <- read.csv("D:\\Buffer\\SDOH__En__1_0 (3).csv", encoding = "UTF-8")
}

run=FALSE

if(run) {
dfBroadband_raw <- read.csv("D:\\RStudio\\CUNY_607\\Projects\\Final Project\\Broadband_Adoption.csv", encoding = "UTF-8")
}
```

The survey instrument and resultant data were created and delivered by an outside agency and needs extensive reforming. The file contains 800,000 observations- the columns are "Field" and "Value".  Each record takes up 211 observations.

We begin by pivoting wider every 211 records, taking our names from "Field" and our values from "Value", and binding each to a final dataframe.

```{r}

run=FALSE

if (run) {
dfSDOH <- dfSDOH_raw %>%
    filter(as.numeric(Num) <=211) %>%
    pivot_wider(Org, names_from = "Field", values_from = "Value")

dfFinal <- dfSDOH

x=211

for (i in 1: 3752)
{

dfSDOH <- dfSDOH_raw %>%
    filter(as.numeric(Num) > x & as.numeric(Num)<= (x+211)) %>%
    pivot_wider(Org, names_from = "Field", values_from = "Value")

x=x+211

dfFinal <- rbind(dfFinal, dfSDOH)
}
}
```

We eliminate bad records that have no ClientID.

```{r}
#This operation is time consuming.  Set run to TRUE the first time you run ii, and FALSE afterward.
run=FALSE

if(run) {
dfFinal %<>%
  filter (ClientID > 0)
}

```

We select the columns which describe needs, and rename them to reflect the type of need. They are: (describe here)

```{r}

NeedsVector = c("Need_Food",  "Need_SafePlace",  "Need_LoseHousing",  "Need_Job",  "Need_AffordNeeds",  "Need_HighSchool",  "Need_HelpUnderstanding",  "Need_Sad",  "Need_Childcare",  "Need_Clothes",  "Need_Transport",  "Need_Safe") 


#This operation is time consuming.  Set run to TRUE the first time you run ii, and FALSE afterward.
run=FALSE

if(run) {

dfFinal2 <- dfFinal %>%
  rename(Need_Food=DoYouOrYou, Need_SafePlace = DoYouHaveA, Need_LoseHousing = AreYouWorr, Need_Job = DoYouHaveA1, Need_AffordNeeds = AreYouAble, Need_HighSchool = DoYouHaveA2, Need_HelpUnderstanding=DoYouEverN, Need_Sad = SocialAreY, Need_Childcare = ChildCareD, Need_Clothes = DoYouEverH, Need_Transport = DoYouHaveA3, Need_Safe = DoYouFeelS )

dfFinal3 <- dfFinal2 %>%
  select(ClientID, starts_with('Need_'))
}

```

Each of the 12 need columns is binary, containing either a yes or no in response to a question about a need. However, sometimes 'yes' means the client has a need ("Do you or your family ever go hungry") and sometimes 'yes' means the client does not have a need ("Do you feel safe physically and emotionally").

We will recode the questions so that 1 always means 'has a need' and 0 always means 'does not have a need'.

```{r}

#This operation is time consuming.  Set run to TRUE the first time you run ii, and FALSE afterward.
run=FALSE

if(run) {
QCorrect<- function(x){
    ifelse(x=="Yes", 1,0)
}

QReverse<- function(x){
    ifelse(x=="No", 1,0)
}
 
dfFinal4 <- dfFinal3 %>%
  select(Need_Food, Need_Sad, Need_HelpUnderstanding, Need_Childcare, Need_Clothes, Need_LoseHousing) %>%
   mutate_all(QCorrect)

dfFinal5 <- dfFinal3 %>%
  select(Need_SafePlace, Need_Job, Need_AffordNeeds, Need_HighSchool, Need_Transport, Need_Safe) %>%
   mutate_all(QReverse)

dfFinal6 <- dfFinal3 %>%
  select(ClientID)

dfFinal6$ClientID <- as.numeric(as.character(dfFinal6$ClientID))

dfFinal10 <- cbind(dfFinal6, dfFinal4, dfFinal5)
}

```


### 4. SQL Server Database Data

The database contains 2 tables: 

a. A table of 5,433 unique clients with demographic data and the client's first A1C result (A1C is a test that indicates diabetes.)
b. A table of 39,491 observations for 8,546 unique clients with A1C, Blood Pressure and other health screen data.

The data is password protected by keyring.  Here we create a connection.

```{r keyring-set-up, echo=FALSE}
#install.packages("keyring")
library(keyring)
#keyring_create("EH_606_keyring")
keyring_unlock("EH_606_keyring")
#key_set(service = "EH_606_username", keyring = "EH_606_keyring")
#key_set(service = "EH_606_pwd", keyring = "EH_606_keyring")
```

```{r db connect, echo=TRUE, results='hide', warning=FALSE, message=FALSE}
# This is the connection string:

strConnection <- paste0(
  'Driver={ODBC Driver 13 for SQL Server};
   Server=tcp:ehtmp.database.windows.net,1433;
   Database=HC_A1C;
   Encrypt=yes;
   TrustServerCertificate=no;
   Connection Timeout=30;',
   'Uid=',keyring::key_get(service = "EH_606_username", keyring = "EH_606_keyring"),';',
   'Pwd=', keyring::key_get(service = "EH_606_pwd", keyring = "EH_606_keyring"), ';'
)

library(RODBC)
dbConnection <- odbcDriverConnect(strConnection)
#keyring_lock
```

## Reading Dataframes into Persistent Storage

If we read our datasets into persistent storage (the SQL Sever database), then we can use SQL to easily combine data from several tables and read the result back into a dataframe. Just as important, many of the datasets in this project are very large and take a long time to load.  Therefore we can filter and prepare them, read them into the database, and then use the database exclusively to read them back tp cut down on load time. 

We begin with the MedicaidByZip file, a NYDOH file with over a million records.  We filter it down to the records we want (the most recent diabetes records from Staten Island) and load it into the database.

```{r}
#This operation is time consuming.  Set run to TRUE the first time you run ii, and FALSE afterward.
run=FALSE
if(run) {

DropTable <- sqlQuery(dbConnection, "DROP TABLE IF EXISTS tblMedicaidByZip")
PopulateTable <- sqlSave(dbConnection, dfMedicaidByZip, "tblMedicaidByZip", append=TRUE)
}

```

Next we clean and load the scraped dataset with 1711 observations by Zip on population and poverty in NY State:

```{r}

#This operation is time consuming.  Set run to TRUE the first time you run ii, and FALSE afterward.
run=FALSE

if(run) {
DropTable <- sqlQuery(dbConnection, "DROP TABLE IF EXISTS tblPopAndPovertyLevel")
PopulateTable <- sqlSave(dbConnection, dfPopAndPovertyLevel, "tblPopAndPovertyLevel", append=TRUE)
}

```

Next we load the Survey data into a table called tblNeeds:

```{r}

#This operation is time consuming.  Set run to TRUE the first time you run ii, and FALSE afterward.
run=FALSE

if(run) {

  
DropTable <- sqlQuery(dbConnection, "DROP TABLE IF EXISTS tblNeeds")
PopulateTable <- sqlSave(dbConnection, dfFinal10, "tblNeeds", append=TRUE)
}

```

Finally, we add the broadband by zip dataframe

```{r}

#This operation is time consuming.  Set run to TRUE the first time you run ii, and FALSE afterward.
run=FALSE

if(run) {

DropTable <- sqlQuery(dbConnection, "DROP TABLE IF EXISTS tblBroadBand")
PopulateTable <- sqlSave(dbConnection, dfBroadband_raw, "tblBroadband", append=TRUE)
}

```

## Create Dataframes from the SQL Server Database

We will create the following dataframes by combining table elements in the database:

1. dfZipCode, a file with information on population info, medicaid Info, broadband info and client Info by zip code
```{r}

dfZipInfo_raw <- sqlQuery(dbConnection, "SELECT tblMedicaidByZip.zip_code, tblMedicaidByZip.major_diagnostic_category, tblPopAndPovertyLevel.population, tblPopAndPovertyLevel.percent_below_poverty, tblMedicaidByZip.ip_admits, tblMedicaidByZip.er_visits, tblA1CAndDemographics.ClientID, tblA1CAndDemographics.Zip, tblA1CAndDemographics.InitialA1C, tblA1CAndDemographics.Gender, tblA1CAndDemographics.RaceandEthnicity, tblA1CAndDemographics.Age, tblBroadband.NoInternetAccessPercentageOfHouseholds
FROM (tblMedicaidByZip INNER JOIN (tblA1CAndDemographics INNER JOIN tblPopAndPovertyLevel ON tblA1CAndDemographics.Zip = tblPopAndPovertyLevel.zip) ON tblMedicaidByZip.zip_code = tblPopAndPovertyLevel.zip) INNER JOIN tblBroadband ON tblPopAndPovertyLevel.zip = tblBroadband.ZipCode;
")

dfZipInfo1 <- dfZipInfo_raw %>%
  filter(major_diagnostic_category=="Diabetes Mellitus") %>%
  select(zip_code, population, percent_below_poverty, ip_admits, er_visits) %>%
  mutate(ip_admits = as.numeric(gsub(",", "", ip_admits))) %>%
    mutate(er_visits = as.numeric(gsub(",", "", er_visits))) %>%
  group_by(zip_code) %>%
  summarize(pop=mean(population), pov=mean(percent_below_poverty), percent_ip = sum(ip_admits/population), percent_er=sum(er_visits/population))

dfZipInfo5 <- dfZipInfo_raw %>%
  filter(major_diagnostic_category=="Diabetes Mellitus") %>%
  select(zip_code, population, NoInternetAccessPercentageOfHouseholds, ip_admits, er_visits) %>%
  mutate(ip_admits = as.numeric(gsub(",", "", ip_admits))) %>%
    mutate(er_visits = as.numeric(gsub(",", "", er_visits))) %>%
  group_by(zip_code) %>%
  summarize(pop=mean(population), NoInternet=mean(NoInternetAccessPercentageOfHouseholds), percent_ip = sum(ip_admits/population), percent_er=sum(er_visits/population))

dfZipInfo3 <- dfZipInfo_raw %>%
  filter(major_diagnostic_category=="Diabetes Mellitus") %>%
  select(zip_code, InitialA1C, percent_below_poverty) %>%
  na.omit() %>%
  group_by(zip_code) %>%
  summarize(n=n(), ave_A1C=mean(InitialA1C), pov=mean(percent_below_poverty))
```
```{r}

dfBroadband <- sqlQuery(dbConnection, "SELECT * FROM tblBroadBand")

```


```{r}
dfA1C <- sqlQuery(dbConnection, "SELECT * FROM tblA1C")
dfA1C %<>%
  mutate(A1CDropPerCent=-1*(MostrecentA1C - DiagA1C)/DiagA1C) %<>%
  mutate(A1CDrop= -1*(MostrecentA1C - DiagA1C)) %>%
  mutate(Improved = case_when(A1CDrop > 0 ~ 1,
                           A1CDrop <= 0 ~ 0))  %>%
  filter(A1CDrop<7.4)


```


```{r}

dfA1CAndDemographics <- sqlQuery(dbConnection, "SELECT * FROM tblA1CAndDemographics")

dfA1CAndDemographics %<>% 
  mutate(BlackOrNot = ifelse(RaceandEthnicity=="Black/African American", 1, 0)) %<>%
  mutate(MaleOrNot = ifelse(Gender=="Male", 1, 0)) %>%
  mutate(A1CAboveMean = ifelse(InitialA1C > 6, 1, 0))

```


```{r}

dfMedicaidByZip<- sqlQuery(dbConnection, "SELECT * FROM tblMedicaidByZip")

dfMedicaidByZip_Staten <- dfMedicaidByZip %>%
  filter(major_diagnostic_category=='Diabetes Mellitus' & county=="Richmond" & year=="2014")

dfErVisitsByZip <- dfMedicaidByZip %>%
  filter(major_diagnostic_category=='Diabetes Mellitus' & year=="2014") %>%
  group_by(zip_code) %>%
  summarize(TotalERVisits = sum(as.numeric(er_visits)))
dfErVisitsByZip


```

```{r}

dfA1cOnNeeds <- sqlQuery(dbConnection, "SELECT tblReadings.ClientID, tblReadings.Reading, AVG(tblNeeds.Need_Food) AS AVGOfNeed_Food, AVG(tblNeeds.Need_Sad) AS AVGOfNeed_Sad, AVG(tblNeeds.Need_HelpUnderstanding) AS AVGOfNeed_HelpUnderstanding, AVG(tblNeeds.Need_Childcare) AS AVGOfNeed_Childcare, AVG(tblNeeds.Need_Clothes) AS AVGOfNeed_Clothes, AVG(tblNeeds.Need_LoseHousing) AS AVGOfNeed_LoseHousing, AVG(tblNeeds.Need_SafePlace) AS AVGOfNeed_SafePlace, AVG(tblNeeds.Need_Job) AS AVGOfNeed_Job, AVG(tblNeeds.Need_AffordNeeds) AS AVGOfNeed_AffordNeeds, AVG(tblNeeds.Need_HighSchool) AS AVGOfNeed_HighSchool, AVG(tblNeeds.Need_Transport) AS AVGOfNeed_Transport, AVG(tblNeeds.Need_Safe) AS AVGOfNeed_Safe
FROM tblReadings INNER JOIN tblNeeds ON tblReadings.ClientID = tblNeeds.ClientID
where tblReadings.[Type]='A1C'
GROUP BY tblReadings.ClientID, tblReadings.Reading")

dfA1cOnNeeds %<>%
  mutate(Reading=as.numeric(Reading)) %<>%
  filter(Reading>4.0)
```


```{r}

dfA1CAndNeeds <- sqlQuery(dbConnection, "SELECT tblReadings.ClientID, tblReadings.Reading, tblNeeds.Need_Food, tblNeeds.Need_Sad, tblNeeds.Need_HelpUnderstanding, tblNeeds.Need_Childcare, tblNeeds.Need_Clothes, tblNeeds.Need_LoseHousing, tblNeeds.Need_SafePlace, tblNeeds.Need_Job, tblNeeds.Need_AffordNeeds, tblNeeds.Need_HighSchool, tblNeeds.Need_Transport, tblNeeds.Need_Safe
FROM tblNeeds INNER JOIN tblReadings ON tblReadings.ClientID = tblNeeds.ClientID
WHERE tblReadings.[Type]='A1C';")

dfA1CAndNeeds <- as.data.frame(lapply(dfA1CAndNeeds, as.numeric))

dfA1CAndNeeds %<>%
  filter(Reading>4.0) %<>%
    group_by(ClientID) %<>%
    summarize(across(everything(), mean, na.rm = TRUE))
```


```{r}




dfZipInfo2 <- dfZipInfo1 %>%
  filter(as.numeric(zip_code) >= 10301 & as.numeric(zip_code) <= 10314)


dfZipInfo4 <- dfZipInfo3 %>%
  filter(as.numeric(zip_code) >= 10301 & as.numeric(zip_code) <= 10314)

dfZipInfo6 <- dfZipInfo5 %>%
  filter(as.numeric(zip_code) >= 10301 & as.numeric(zip_code) <= 10314)

ggplot(dfZipInfo2, aes(pov, percent_ip)) +
  geom_point()

rBase1 <- lm( percent_ip ~ pov,  data = dfZipInfo2)
summary(rBase1)


ggplot(dfZipInfo2, aes(pov, percent_er)) +
  geom_point()

rBase1 <- lm(percent_er ~ pov,  data = dfZipInfo2)
summary(rBase1)


ggplot(dfZipInfo4, aes(pov, ave_A1C)) +
  geom_point()

rBase1 <- lm(ave_A1C ~ pov,  data = dfZipInfo4)
summary(rBase1)

ggplot(dfZipInfo6, aes(NoInternet, percent_er)) +
  geom_point()

rBase1 <- lm(percent_er ~ NoInternet,  data = dfZipInfo6)
summary(rBase1)

```


```{r}


dfA1CAndNeeds$Reading <- as.numeric(dfA1CAndNeeds$Reading)

rBase1 <- lm( Reading ~ Need_Food + Need_Sad + Need_HelpUnderstanding + Need_Childcare + Need_Clothes + Need_LoseHousing + Need_SafePlace + Need_Job + Need_AffordNeeds + Need_HighSchool + Need_Transport + Need_Safe,  data = dfA1CAndNeeds)
summary(rBase1)

rBase2 <- lm( Reading ~ Need_AffordNeeds,  data = dfA1CAndNeeds)
summary(rBase2)


```
```{r}

#install.packages('psych')

psych::describe(dfA1CAndNeeds)

```


```{r}



dfA1CDropAndNeeds <- sqlQuery(dbConnection, "SELECT tblA1C.DiagA1C, tblA1C.MostrecentA1C, tblNeeds.Need_Food, tblNeeds.Need_Sad, tblNeeds.Need_HelpUnderstanding, tblNeeds.Need_Childcare, tblNeeds.Need_Clothes, tblNeeds.Need_LoseHousing, tblNeeds.Need_SafePlace, tblNeeds.Need_Job, tblNeeds.Need_AffordNeeds, tblNeeds.Need_HighSchool, tblNeeds.Need_Transport, tblNeeds.Need_Safe
FROM tblA1C INNER JOIN tblNeeds ON tblA1C.ClientID = tblNeeds.ClientID;")

dfA1CDropAndNeeds %<>% 
  mutate(A1CDrop = MostrecentA1C-DiagA1C)

```




```{r}

dfChronicAndNeeds <- sqlQuery(dbConnection, "SELECT tblNeeds.ClientID, tblChronic.Chronic, tblReadings.Type,  tblNeeds.Need_Food, tblNeeds.Need_Sad, tblNeeds.Need_HelpUnderstanding, tblNeeds.Need_Childcare, tblNeeds.Need_Clothes, tblNeeds.Need_LoseHousing, tblNeeds.Need_SafePlace, tblNeeds.Need_Job, tblNeeds.Need_AffordNeeds, tblNeeds.Need_HighSchool, tblNeeds.Need_Transport, tblNeeds.Need_Safe
FROM tblReadings INNER JOIN (tblChronic RIGHT JOIN tblNeeds ON tblChronic.ClientID = tblNeeds.ClientID) ON tblReadings.ClientID = tblNeeds.ClientID
WHERE tblReadings.Type='Blood Pressure' Or tblReadings.Type='A1C';")


#dfChronicAndNeeds <- as.data.frame(lapply(dfChronicAndNeeds, as.numeric))

dfDiabetesAndNeeds <- dfChronicAndNeeds %>%
    group_by(ClientID) %>%
    mutate(DiabetesOrNot = ifelse(Chronic == "Diabetes Type I"|Chronic=="Diabetes Type II", 1,0)) %>%
    summarize(sum(DiabetesOrNot), across(NeedsVector, mean, na.rm = TRUE)) %>%
    rename(DiabetesOrNot="sum(DiabetesOrNot)") %>%
    mutate(DiabetesOrNot = ifelse(DiabetesOrNot>0, 1, 0))

    dfDiabetesAndNeeds$DiabetesOrNot[is.na(dfDiabetesAndNeeds$DiabetesOrNot)] <- 0
    
dfAllChronicAndNeeds <- dfChronicAndNeeds %>%
    group_by(ClientID) %>%
    mutate(DiabetesOrNot = ifelse(Chronic == "Diabetes Type I"|Chronic=="Diabetes Type II", 1,0)) %>%
    mutate(HypertensionOrNot = ifelse(Chronic == "Hypertension", 1,0)) %>%
    mutate(HypertensionOrDiabetesOrNot = ifelse(Chronic == "Diabetes Type I"|Chronic=="Diabetes Type II"|Chronic == "Hypertension", 1,0)) %>%
    summarize(sum(DiabetesOrNot), sum(HypertensionOrNot), sum(HypertensionOrDiabetesOrNot), across(NeedsVector, mean, na.rm = TRUE)) %>%
    rename(DiabetesOrNot="sum(DiabetesOrNot)") %>%
      rename(HypertensionOrNot="sum(HypertensionOrNot)") %>%
      rename(HypertensionOrDiabetesOrNot="sum(HypertensionOrDiabetesOrNot)") %>%
      mutate(DiabetesOrNot = ifelse(DiabetesOrNot>0, 1, 0)) %>%
      mutate(HypertensionOrNot = ifelse(HypertensionOrNot>0, 1, 0)) %>%
      mutate(HypertensionOrDiabetesOrNot = ifelse(HypertensionOrDiabetesOrNot>0, 1, 0)) %>%
      mutate(NumOfNeeds = Need_Sad+Need_HelpUnderstanding+Need_Childcare+Need_Clothes+ Need_LoseHousing+Need_SafePlace+ Need_Job+Need_AffordNeeds+Need_HighSchool+Need_Transport+Need_Safe)

dfAllChronicAndNeedsMinusNA <- na.omit(dfAllChronicAndNeeds)
  

    dfAllChronicAndNeeds$DiabetesOrNot[is.na(dfAllChronicAndNeeds$DiabetesOrNot)] <- 0
    dfAllChronicAndNeeds$HypertensionOrNot[is.na(dfAllChronicAndNeeds$HypertensionOrNot)] <- 0
    dfAllChronicAndNeeds$HypertensionOrDiabetesOrNot[is.na(dfAllChronicAndNeeds$HypertensionOrDiabetesOrNot)] <- 0
    
 psych::describe(dfAllChronicAndNeeds) 
```

```{r}

m10 <- glm(DiabetesOrNot ~ Need_Food + Need_Sad + Need_HelpUnderstanding + Need_Childcare + Need_Clothes + Need_LoseHousing + Need_SafePlace + Need_Job + Need_AffordNeeds + Need_HighSchool + Need_Transport + Need_Safe, family = "binomial", data=dfAllChronicAndNeeds)
summary(m10)

m10 <- glm(HypertensionOrNot ~ Need_Food + Need_Sad + Need_HelpUnderstanding + Need_Childcare + Need_Clothes + Need_LoseHousing + Need_SafePlace + Need_Job + Need_AffordNeeds + Need_HighSchool + Need_Transport + Need_Safe, family = "binomial", data=dfAllChronicAndNeeds)
summary(m10)

m10 <- glm(HypertensionOrDiabetesOrNot ~ Need_Food + Need_Sad + Need_HelpUnderstanding + Need_Childcare + Need_Clothes + Need_LoseHousing + Need_SafePlace + Need_Job + Need_AffordNeeds + Need_HighSchool + Need_Transport + Need_Safe, family = "binomial", data=dfAllChronicAndNeeds)
summary(m10)

m10 <- glm(DiabetesOrNot ~ Need_AffordNeeds, family = "binomial", data=dfAllChronicAndNeeds)
summary(m10)

x2 <- chisq.test(dfAllChronicAndNeeds$DiabetesOrNot, dfAllChronicAndNeeds$Need_AffordNeeds)
x2

x2 <- chisq.test(dfAllChronicAndNeeds$Need_HighSchool, dfAllChronicAndNeeds$Need_Job)
x2

table(dfAllChronicAndNeeds$DiabetesOrNot, dfAllChronicAndNeeds$Need_AffordNeeds)

table(dfAllChronicAndNeeds$HypertensionOrDiabetesOrNot, dfAllChronicAndNeeds$Need_Clothes)
table(dfAllChronicAndNeeds$HypertensionOrDiabetesOrNot, dfAllChronicAndNeeds$Need_LoseHousing)
table(dfAllChronicAndNeeds$HypertensionOrDiabetesOrNot, dfAllChronicAndNeeds$Need_AffordNeeds)
table(dfAllChronicAndNeeds$HypertensionOrDiabetesOrNot, dfAllChronicAndNeeds$Need_Job)
table(dfAllChronicAndNeeds$HypertensionOrDiabetesOrNot, dfAllChronicAndNeeds$Need_HighSchool)

```

```{r}

#https://www.statology.org/logistic-regression-in-r/
#make this example reproducible
set.seed(1)

#Use 70% of dfAllChronicAndNeedsset as training set and remaining 30% as testing set
sample <- sample(c(TRUE, FALSE), nrow(dfAllChronicAndNeeds), replace=TRUE, prob=c(0.7,0.3))
train <- dfAllChronicAndNeeds[sample, ]
test <- dfAllChronicAndNeeds[!sample, ]  

#fit logistic regression model
model <- glm(HypertensionOrDiabetesOrNot ~ Need_Job + Need_AffordNeeds + Need_HighSchool, family="binomial", data=train)
summary(model)

model <- glm(HypertensionOrDiabetesOrNot ~ Need_HighSchool + Need_AffordNeeds, family="binomial", data=train)
summary(model)

model1 <- glm(HypertensionOrDiabetesOrNot~ Need_Sad + Need_HelpUnderstanding + Need_Childcare + Need_Clothes + Need_LoseHousing + Need_SafePlace + Need_Job + Need_AffordNeeds + Need_HighSchool + Need_Transport + Need_Safe, family = "binomial", data=train)
summary(model1)

model <- glm(DiabetesOrNot~ Need_Sad + Need_HelpUnderstanding + Need_Childcare + Need_Clothes + Need_LoseHousing + Need_SafePlace + Need_Job + Need_AffordNeeds + Need_HighSchool + Need_Transport + Need_Safe, family = "binomial", data=train)
summary(model)

#model <- glm(HypertensionOrDiabetesOrNot ~ NumOfNeeds, family="binomial", data=train)
#model
#disable scientific notation for model summary
#options(scipen=999)

#view model summary
summary(model)
#Shows fit - ranges from 0 to almost 1
pscl::pR2(model)["McFadden"]

#Ranks importance
caret::varImp(model)

#multicolinearity (VIF)
car::vif(model)

#calculate probability of default for each individual in test dataset
predicted <- predict(model, test, type="response")
p2 <- as.data.frame(predicted)

p3 <- test %>%
  select(HypertensionOrDiabetesOrNot)

Predictions <- cbind(p2, p3)
Predictions$predicted <- ifelse(Predictions$predicted >.13, 1, 0)
Predictions$predicted=as.factor(Predictions$predicted)
Predictions$HypertensionOrDiabetesOrNot=as.factor(Predictions$HypertensionOrDiabetesOrNot)

psych::describe(Predictions)

#find optimal cutoff probability to use to maximize accuracy
optimal <- InformationValue::optimalCutoff(test$HypertensionOrDiabetesOrNot, predicted)[1]
optimal

#ConfusionMatrix doesn't work
confusionMatrix(Predictions$HypertensionOrDiabetesOrNot, Predictions$predicted)

#calculate sensitivity
#sensitivity(test$HypertensionOrDiabetesOrNot, predicted)

#calculate specificity
#specificity(test$HypertensionOrDiabetesOrNot, predicted)

#calculate total misclassification error rate
#misClassError(Predictions$HypertensionOrDiabetesOrNot, Predictions$predicted, threshold=optimal)


```

```{r}
set.seed(1)

#Use 70% of dfAllChronicAndNeedsset as training set and remaining 30% as testing set
sample <- sample(c(TRUE, FALSE), nrow(dfAllChronicAndNeeds), replace=TRUE, prob=c(0.7,0.3))
train <- dfAllChronicAndNeeds[sample, ]
test <- dfAllChronicAndNeeds[!sample, ]  

#fit logistic regression model

model <- glm(DiabetesOrNot~ Need_Sad + Need_HelpUnderstanding + Need_Childcare + Need_Clothes + Need_LoseHousing + Need_SafePlace + Need_Job + Need_AffordNeeds + Need_HighSchool + Need_Transport + Need_Safe, family = "binomial", data=train)
summary(model)
#model <- glm(HypertensionOrDiabetesOrNot ~ NumOfNeeds, family="binomial", data=train)
#model
#disable scientific notation for model summary
#options(scipen=999)

#view model summary
summary(model)
#Shows fit - ranges from 0 to almost 1
pscl::pR2(model)["McFadden"]

#Ranks importance
caret::varImp(model)

#multicolinearity (VIF)
car::vif(model)

#calculate probability of default for each individual in test dataset
predicted <- predict(model, test, type="response")
p2 <- as.data.frame(predicted)

p3 <- test %>%
  select(DiabetesOrNot)

Predictions <- cbind(p2, p3)
Predictions$predicted <- ifelse(Predictions$predicted >.13, 1, 0)
Predictions$predicted=as.factor(Predictions$predicted)
Predictions$DiabetesOrNot=as.factor(Predictions$DiabetesOrNot)

psych::describe(Predictions)

#find optimal cutoff probability to use to maximize accuracy
optimal <- InformationValue::optimalCutoff(test$DiabetesOrNot, predicted)[1]
optimal

#ConfusionMatrix doesn't work
caret::confusionMatrix(Predictions$DiabetesOrNot, Predictions$predicted)

#calculate sensitivity
#sensi



```

```{r}
set.seed(1)

#Use 70% of dfAllChronicAndNeedsset as training set and remaining 30% as testing set
sample <- sample(c(TRUE, FALSE), nrow(dfA1CDropAndNeeds), replace=TRUE, prob=c(0.7,0.3))
train <- dfA1CDropAndNeeds[sample, ]
test <- dfA1CDropAndNeeds[!sample, ]  

#fit logistic regression model

model <- lm(A1CDrop~ Need_Sad + Need_HelpUnderstanding + Need_Childcare + Need_Clothes + Need_LoseHousing + Need_SafePlace + Need_Job + Need_AffordNeeds + Need_HighSchool + Need_Transport + Need_Safe, data=train)
summary(model)
#model <- glm(HypertensionOrA1CDrop ~ NumOfNeeds, family="binomial", data=train)
#model
#disable scientific notation for model summary
#options(scipen=999)

#view model summary
summary(model)
#Shows fit - ranges from 0 to almost 1
pscl::pR2(model)["McFadden"]

#Ranks importance
caret::varImp(model)

#multicolinearity (VIF)
#car::vif(model)

#calculate probability of default for each individual in test dataset
predicted <- predict(model, test, type="response")
p2 <- as.data.frame(predicted)

p3 <- test %>%
  select(A1CDrop)

Predictions <- cbind(p2, p3)
Predictions$predicted <- ifelse(Predictions$predicted >.13, 1, 0)
Predictions$predicted=as.factor(Predictions$predicted)
Predictions$A1CDrop=as.factor(Predictions$A1CDrop)

psych::describe(Predictions)

#find optimal cutoff probability to use to maximize accuracy
optimal <- InformationValue::optimalCutoff(test$A1CDrop, predicted)[1]
optimal

#ConfusionMatrix doesn't work#confusionMatrix(Predictions$A1CDrop, Predictions$predicted)

#calculate sensitivity
#sensi



```


```{r}


set.seed(1)

#Use 70% of dfAllChronicAndNeedsset as training set and remaining 30% as testing set
sample <- sample(c(TRUE, FALSE), nrow(dfAllChronicAndNeedsMinusNA), replace=TRUE, prob=c(0.7,0.3))
train <- dfAllChronicAndNeedsMinusNA[sample, ]
test <- dfAllChronicAndNeedsMinusNA[!sample, ]  

#fit logistic regression model

model <- glm(HypertensionOrDiabetesOrNot~  Need_Clothes + Need_Job + Need_AffordNeeds, family = "binomial", data=train)
summary(model)
#model <- glm(HypertensionOrDiabetesOrNot ~ NumOfNeeds, family="binomial", data=train)
#model
#disable scientific notation for model summary
#options(scipen=999)

#view model summary
summary(model)
#Shows fit - ranges from 0 to almost 1
pscl::pR2(model)["McFadden"]

#Ranks importance
caret::varImp(model)

#multicolinearity (VIF)
car::vif(model)

#calculate probability of default for each individual in test dataset
predicted <- predict(model, test, type="response")
p2 <- as.data.frame(predicted)

p3 <- test %>%
  select(HypertensionOrDiabetesOrNot)

Predictions <- cbind(p2, p3)
Predictions$predicted <- ifelse(Predictions$predicted >.755, 1, 0)
Predictions$predicted=as.factor(Predictions$predicted)
Predictions$HypertensionOrDiabetesOrNot=as.factor(Predictions$HypertensionOrDiabetesOrNot)

psych::describe(Predictions)

#find optimal cutoff probability to use to maximize accuracy
optimal <- InformationValue::optimalCutoff(test$HypertensionOrDiabetesOrNot, predicted)[1]
optimal

#ConfusionMatrix doesn't work
caret::confusionMatrix(Predictions$HypertensionOrDiabetesOrNot, Predictions$predicted)

#calculate sensitivity
#sensi


```


```{r}


psych::describe(dfA1CAndDemographics)

rBase1 <- lm(InitialA1C ~ BlackOrNot + MaleOrNot + Age,  data = dfA1CAndDemographics)
summary(rBase1)

```

```{r}


set.seed(1)

#Use 70% of dfAllChronicAndNeedsset as training set and remaining 30% as testing set
sample <- sample(c(TRUE, FALSE), nrow(dfA1CAndDemographics), replace=TRUE, prob=c(0.7,0.3))
train <- dfA1CAndDemographics[sample, ]
test <- dfA1CAndDemographics[!sample, ]  

#fit logistic regression model

model <- glm(A1CAboveMean ~ BlackOrNot + MaleOrNot + Age, family = "binomial", data=train)
#disable scientific notation for model summary
options(scipen=999)

#view model summary
summary(model)
#Shows fit - ranges from 0 to almost 1
pscl::pR2(model)["McFadden"]

#Ranks importance
caret::varImp(model)

#multicolinearity (VIF)
car::vif(model)

#calculate probability of default for each individual in test dataset
predicted <- predict(model, test, type="response")
p2 <- as.data.frame(predicted)

p3 <- test %>%
  select(A1CAboveMean)

Predictions <- cbind(p2, p3)
Predictions$predicted <- ifelse(Predictions$predicted >.69, 1, 0)
Predictions$predicted=as.factor(Predictions$predicted)
Predictions$A1CAboveMean=as.factor(Predictions$A1CAboveMean)

psych::describe(Predictions)

#find optimal cutoff probability to use to maximize accuracy
optimal <- InformationValue::optimalCutoff(test$A1CAboveMean, predicted)[1]
optimal

#ConfusionMatrix doesn't work
caret::confusionMatrix(Predictions$A1CAboveMean, Predictions$predicted)

#calculate sensitivity
#sensi


```

